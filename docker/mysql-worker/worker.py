from webbrowser import get
import boto3
import os
import time
import json
import sys

"""
Demo script to show working with AWS StepFunctions and AWS ECS/Fargate tasks

Logic:
AWS StepFunction is called defining:
  - Type of task to run (db_backup or db_restore)
  - Required attributes (DB info and S3 paths, VPC networking so task can reach the database)
  - Task token (generated by StepFunction) so this script can report back its status
AWS ECS task then:
  - reads in the passed variables (set as ENV variables on task launch by StepFunction)
  - branches appropriately (back or restore)
  - dumps MySQL DB (only supported engine for this demo) locally
  - uploads the dump to S3
  - reports success or failure back to the calling StepFunction

Demo limitations:
As this is to show the flow and functionality, below are some areas to consider before extending this for production.
- Local storage: Tasks start with ~20G of storage, although up to ~200G can be specified (varies as the total also includes the size of the container image)
- Tasks support EFS attachments, it may be desirable to add at least a single-zone EFS for dumps to write to before being uploaded to S3
  - If using EFS, add logic to delete the dump after its uploaded to save costs
- Timeouts: StepFunctions by default wait 1 year for the task complete before terminating it. Any issues with this task may cause the SF to persist that long.
  - For the demo, the associated StepFunction has been set to time out after 600 seconds.
  - StepFunctions support a heartbeat for the task to send back letting it know its still in progress. This should be used before implementing this fully.
- Status: as this task calls the mysql commands, it is not tracking the progress of the job for reporting back
  - If a task is long running, i.e. a large backup, capturing the mysql output and determing a status and ETA should be done
  - Such status could then be reported, such as to DynamoDB, so that an associated "get_status" API call can be performed by those invoking this function

Best practices changes:
- The functions here would likely be common to more tasks and should be modules imported by this worker.py instead of written here.
"""



def send_error(error_cause, error_msg):
    """Send error back to calling StepFunction"""

    client = boto3.client('stepfunctions')
     # see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/stepfunctions.html#SFN.Client.send_task_failure
    response = client.send_task_failure(
         taskToken=stepfunction_token,
         error=str(error_msg),  # Since passing in "e" for debugging of the exception, ensure these are strings
         cause=str(error_cause)
    )
    print(f"Error has occured, aborting. Cause: {error_cause}, message: {error_msg}")
    sys.exit(1)

def send_success(output):
    """Send success and output back to calling StepFunction"""
    
    client = boto3.client('stepfunctions')
    # see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/stepfunctions.html#SFN.Client.send_task_success
    response = client.send_task_success(
        taskToken=stepfunction_token,
        output=json.dumps(output)
    )

# send heartbeat to stepfunction
# def send_heartbeat():
    #"""Send keep-alive back to calling StepFunction so this task isn't forcibly killed if reaching time-out setting"""
    # TBD, will be needed if this will be a long running task (we should set SF to timeout or it'll wait 1year)
    #client = boto3.client('stepfunctions')
    #response = client.send_task_heartbeat(
    #    taskToken=stepfunction_token
    #)

# s3 file copy
# def s3_copy(source, dest):
#     #TBD

def get_parameter(keyname):
    ssm = boto3.client('ssm')
    try:
        response = ssm.get_parameter(Name=keyname,WithDecryption=True)
        return response['Parameter']['Value']
    except Exception as e:
        send_error(e, "Error trying to retrieve parameter " + keyname + " from Parameter Store")


def db_backup(db_host, db_port, db_user, db_pass, db_name, s3_bucket, s3_path):
    """Perform MySQL backup locally and call S3 copy function"""
    timestamp = time.strftime('%Y-%m-%d-%I')
    # dump_name = db_name+"_"+timestamp+".sql"
    # os.popen(f'mysqldump -h {db_host} -P {db_port} -u {db_user} -p{db_pass} > {dump_name}')

    #s3_copy(dump_name, "s3://"" + s3_bucket + "/" + s3_path)

    output['status']="job complete"
    output['message']="Database " + db_name + " from host " + "db_host backed up on " + timestamp
    send_success(output)

# Main logic when called
if __name__=="__main__":
    # Get common required ENV variables
    try:
        stepfunction_token = os.environ['TASK_TOKEN_ENV_VARIABLE']
    except:
        print("Environment variable TASK_TOKEN_ENV_VARIABLE is missing, aborting")
        sys.exit(1)

    try:
        job_name = os.environ['JOB_NAME']
        #print(f"DEBUG: job name is {job_name}")
    except Exception as e:
        print("Environment variable JOB_NAME is missing, aborting")
        send_error(e, "Environment variable JOB_NAME is missing, aborting")
        #sys.exit(1)

    # Initializing the output dictionary (I hate blank messages when debugging)
    output = {
        "status": "none yet",
        "message": "output initialized, not populated"
    }

    # Parse job name and branch appropriately
    if job_name.lower() == 'db_backup':
        try: # get required env vars
            db_name = os.environ['DB_NAME'] # for more general/multi-tenant use, we probably want to base this on a unique key instead for Parameter Store lookups
        except Exception as e:
            # print("Error trying to get env var DB_NAME")
            send_error(e, "Required parameters (DB_NAME) were not set, task aborted")

        try:
            # if passed in, use env vars, useful for debugging
            db_host = os.environ['DB_HOST']
            db_port = os.environ['DB_PORT']
            db_user = os.environ['DB_USER']
            db_pass = os.environ['DB_PASS']
            s3_bucket = os.environ['S3_BUCKET']
            s3_path = os.environ['S3_PATH']
        except:
            # if no env vars, get from parameter store, for productionizing this, maybe make this the default
            # hard-coding the Parameter Store keypath for now. Using /pubdemo/databases as root and then /databasename as the path holding the rest of the values
            print("No env vars for DB info, checking Parameter Store")
            try:
                keybase = "/publisherdemo/databases/" + db_name # this should probably be passed a passed-in unique ID instead, otherwise no two DB instances can have the same DB name
                db_host = get_parameter(keybase+ "/db_host")
                db_port = get_parameter(keybase+ "/db_port")
                db_user = get_parameter(keybase+ "/db_user")
                db_pass = get_parameter(keybase+ "/db_pass")
                s3_bucket = get_parameter(keybase+ "/s3_bucket")
                s3_path = get_parameter(keybase+ "/s3_path")
                print(db_host)
            except Exception as e:
                #send_error(e, "Error trying to get Parameter Store entries")
                print("issue using parameter store")
                sys.exit(e)

        else:
            print("Backing up the DB")
            db_backup(db_host, db_port, db_user, db_pass, db_name, s3_bucket, s3_path)
    elif job_name.lower() == 'db_restore':
        print("asked to restore but this isn't coded yet")
        send_error("Unwritten function called", "Not implemented")
    else:
        # abort logic and send error to SF
        print("no valid job mentioned")
        send_error("Invalid job name", "A required JOB_NAME env variable was not set, valid values are: db_backup, db_restore")